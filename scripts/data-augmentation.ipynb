{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Churn Analysis and Data Augmentation Notebook\n",
    "\n",
    "## README\n",
    "\n",
    "This notebook performs customer churn analysis and data augmentation on a customer dataset. It includes the following main components:\n",
    "\n",
    "1. Data Loading: Loads the original training and testing datasets.\n",
    "2. Feature Engineering: Adds new features to the dataset to enhance the analysis.\n",
    "3. Data Augmentation: Creates synthetic data to expand the dataset, including:\n",
    "   - Adding noise to numerical features\n",
    "   - Introducing anomalies\n",
    "   - Adding missing values\n",
    "4. Visualization: Generates various plots to compare the original and augmented datasets, including:\n",
    "   - Distribution comparisons\n",
    "   - Correlation heatmaps\n",
    "   - Churn rate by categorical features\n",
    "   - Feature importance\n",
    "   - Scatter plots and box plots of key features\n",
    "\n",
    "### How to Use\n",
    "\n",
    "1. Ensure you have the required libraries installed (see requirements.txt).\n",
    "2. Update the file paths in the `main()` function to point to your data files.\n",
    "3. Run the entire notebook.\n",
    "4. Examine the printed data samples and shapes, as well as the generated visualizations.\n",
    "\n",
    "### Key Functions\n",
    "\n",
    "- `load_data()`: Loads the datasets\n",
    "- `add_features()`: Adds new features to the data\n",
    "- `augment_data()`: Creates an augmented dataset\n",
    "- `create_visualizations()`: Generates various plots for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data and create additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_data_path = '../data/customer_churn_dataset-training-master.csv'\n",
    "test_data_path = '../data/customer_churn_dataset-testing-master.csv'\n",
    "\n",
    "\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "test_data = pd.read_csv(test_data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(data):\n",
    "    \"\"\"\n",
    "    Add new features to the dataset.\n",
    "    \"\"\"\n",
    "    # Tenure in Years\n",
    "    data['Tenure_Years'] = data['Tenure'] / 12\n",
    "    \n",
    "    # Monthly Spend\n",
    "    data['Monthly_Spend'] = data['Total Spend'] / data['Tenure']\n",
    "    \n",
    "    # Interaction Frequency\n",
    "    data['Interaction_Frequency'] = data['Usage Frequency'] / data['Tenure']\n",
    "    \n",
    "    # Support Call Rate\n",
    "    data['Support_Call_Rate'] = data['Support Calls'] / data['Tenure']\n",
    "    \n",
    "    # Payment Delay Rate\n",
    "    data['Payment_Delay_Rate'] = data['Payment Delay'] / data['Tenure']\n",
    "    \n",
    "    # Customer Loyalty\n",
    "    data['Loyal_Customer'] = data['Tenure'].apply(lambda x: 1 if x > 24 else 0)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample data and create synthetic data with noise and additional missing values - to make it a bit more intersting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_data(original_data, synth_size):\n",
    "    \"\"\"\n",
    "    Create synthetic data by sampling from the original data and adding noise.\n",
    "    \"\"\"\n",
    "    # Sample rows from the original data with replacement\n",
    "    synthetic_data = original_data.sample(n=synth_size, replace=True).reset_index(drop=True)\n",
    "    \n",
    "    # Add noise to numerical columns\n",
    "    noise_factor = 0.05  # 5% random noise\n",
    "    numeric_columns = ['Age', 'Tenure', 'Usage Frequency', 'Support Calls', 'Payment Delay', \n",
    "                       'Total Spend', 'Last Interaction', 'Tenure_Years', 'Monthly_Spend', \n",
    "                       'Interaction_Frequency', 'Support_Call_Rate', 'Payment_Delay_Rate']\n",
    "    \n",
    "    for column in numeric_columns:\n",
    "        synthetic_data[column] = synthetic_data[column] * (1 + noise_factor * np.random.randn(synth_size))\n",
    "    \n",
    "    return synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduce_anomalies(data, anomaly_ratio=0.01):\n",
    "    \"\"\"\n",
    "    Introduce anomalies by setting extreme values for certain features.\n",
    "    \"\"\"\n",
    "    anomaly_indices = np.random.choice(len(data), size=int(anomaly_ratio * len(data)), replace=False)\n",
    "    data.loc[anomaly_indices, 'Support Calls'] = data['Support Calls'].max() * 5\n",
    "    data.loc[anomaly_indices, 'Total Spend'] = data['Total Spend'].max() * 10\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduce_missing_values(data, missing_ratio=0.03):\n",
    "    \"\"\"\n",
    "    Introduce missing values in specified columns.\n",
    "    \"\"\"\n",
    "    columns_with_missing = ['Age', 'Gender', 'Tenure', 'Payment Delay', 'Last Interaction']\n",
    "    for column in columns_with_missing:\n",
    "        missing_indices = np.random.choice(len(data), size=int(missing_ratio * len(data)), replace=False)\n",
    "        data.loc[missing_indices, column] = np.nan\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(original_data, augmentation_factor=9):\n",
    "    \"\"\"\n",
    "    Create an augmented dataset by combining original and synthetic data.\n",
    "    \"\"\"\n",
    "    synth_size = len(original_data) * augmentation_factor\n",
    "    synthetic_data = create_synthetic_data(original_data, synth_size)\n",
    "    synthetic_data = introduce_anomalies(synthetic_data)\n",
    "    synthetic_data = introduce_missing_values(synthetic_data)\n",
    "    augmented_data = pd.concat([original_data, synthetic_data], axis=0).reset_index(drop=True)\n",
    "    return augmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new features\n",
    "train_data = add_features(train_data)\n",
    "test_data = add_features(test_data)\n",
    "\n",
    "# Create augmented dataset\n",
    "augmented_data = augment_data(train_data)\n",
    "\n",
    "# Print sample of original and augmented data\n",
    "print(\"Original Data Sample:\")\n",
    "print(train_data.head())\n",
    "print(\"\\nAugmented Data Sample:\")\n",
    "print(augmented_data.head())\n",
    "\n",
    "# Print shape of original and augmented data\n",
    "print(f\"\\nOriginal Data Shape: {train_data.shape}\")\n",
    "print(f\"Augmented Data Shape: {augmented_data.shape}\")\n",
    "\n",
    "augmented_data.to_csv(\"../data/augmented-training data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data.to_csv(\"../data/augmented-training data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the original and augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style for all plots\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_distributions(original_data, augmented_data, columns, n_rows, n_cols):\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5*n_rows))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(columns):\n",
    "        sns.kdeplot(data=original_data, x=col, ax=axes[i], label='Original', color='blue')\n",
    "        sns.kdeplot(data=augmented_data, x=col, ax=axes[i], label='Augmented', color='red')\n",
    "        axes[i].set_title(f'Distribution of {col}')\n",
    "        axes[i].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_heatmap(data, title):\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    corr = data.select_dtypes(include=[np.number]).corr()\n",
    "    sns.heatmap(corr, annot=False, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_churn_rate_by_category(data, category, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=category, y='Churn', data=data, estimator=lambda x: (x == 1).mean() * 100)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Churn Rate (%)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(data):\n",
    "\n",
    "    # Prepare the data\n",
    "    X = data.drop('Churn', axis=1)\n",
    "    y = data['Churn']\n",
    "\n",
    "    # Encode categorical variables\n",
    "    le = LabelEncoder()\n",
    "    X['Gender'] = le.fit_transform(X['Gender'])\n",
    "\n",
    "    # Train a Random Forest model\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "\n",
    "    # Plot feature importance\n",
    "    feature_importance = pd.DataFrame({'feature': X.columns, 'importance': rf.feature_importances_})\n",
    "    feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='importance', y='feature', data=feature_importance)\n",
    "    plt.title('Feature Importance')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distributions of key numerical features\n",
    "numerical_features = ['Age', 'Tenure', 'Usage Frequency', 'Total Spend', 'Monthly_Spend', 'Interaction_Frequency']\n",
    "compare_distributions(train_data, augmented_data, numerical_features, 2, 3)\n",
    "\n",
    "# Plot correlation heatmaps\n",
    "plot_correlation_heatmap(train_data, 'Correlation Heatmap - Original Data')\n",
    "plot_correlation_heatmap(augmented_data, 'Correlation Heatmap - Augmented Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot churn rate by categorical features\n",
    "categorical_features = ['Gender', 'Loyal_Customer']\n",
    "for feature in categorical_features:\n",
    "    plot_churn_rate_by_category(train_data, feature, f'Churn Rate by {feature} - Original Data')\n",
    "    plot_churn_rate_by_category(augmented_data, feature, f'Churn Rate by {feature} - Augmented Data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of Total Spend vs Tenure, colored by Churn\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(data=train_data, x='Total Spend', y='Tenure', hue='Churn', alpha=0.6)\n",
    "plt.title('Total Spend vs Tenure (Original Data)')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(data=augmented_data, x='Total Spend', y='Tenure', hue='Churn', alpha=0.6)\n",
    "plt.title('Total Spend vs Tenure (Augmented Data)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot of Monthly Spend by Churn status\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Churn', y='Monthly_Spend', data=train_data)\n",
    "plt.title('Monthly Spend by Churn Status (Original Data)')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Churn', y='Monthly_Spend', data=augmented_data)\n",
    "plt.title('Monthly Spend by Churn Status (Augmented Data)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
